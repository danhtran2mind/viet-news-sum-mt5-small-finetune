{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-21T12:14:34.305542Z",
     "iopub.status.busy": "2025-03-21T12:14:34.305309Z",
     "iopub.status.idle": "2025-03-21T12:14:40.549531Z",
     "shell.execute_reply": "2025-03-21T12:14:40.547562Z",
     "shell.execute_reply.started": "2025-03-21T12:14:34.305520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T01:54:54.904741Z",
     "iopub.status.busy": "2025-03-21T01:54:54.904466Z",
     "iopub.status.idle": "2025-03-21T01:55:19.407485Z",
     "shell.execute_reply": "2025-03-21T01:55:19.406576Z",
     "shell.execute_reply.started": "2025-03-21T01:54:54.904720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import datasets\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"google/mt5-small\" \n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)  \n",
    "\n",
    "config = T5ForConditionalGeneration.from_pretrained(model_name).config\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name,config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_name = \"OpenHust/vietnamese-summarization\"\n",
    "dataset = datasets.load_dataset(dataset_name)\n",
    "\n",
    "# Define a function to preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"Document\"]\n",
    "    targets = examples[\"Summary\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding in the loss\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "tokenized_datasets = tokenized_datasets['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Rename the splits to 'train' and 'val'\n",
    "tokenized_datasets['val'] = tokenized_datasets.pop('test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset from the local directory\n",
    "local_dataset_dir = \"dataset/raw_dataset\"\n",
    "\n",
    "# Save the dataset to the local directory\n",
    "dataset.save_to_disk(local_dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_dataset_tokenizer_dir = \"dataset/tokenizers\"\n",
    "os.makedirs(local_dataset_tokenizer_dir, exist_ok=True)\n",
    "\n",
    "# Save the preprocessed dataset to the \"dataset\" folder\n",
    "tokenized_datasets.save_to_disk(local_dataset_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset from the local directory\n",
    "dataset = dataset.load_from_disk(\"dataset/raw_dataset\")\n",
    "\n",
    "print(\"Dataset loaded from local directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Tokernizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T01:55:25.962061Z",
     "iopub.status.busy": "2025-03-21T01:55:25.961831Z",
     "iopub.status.idle": "2025-03-21T01:55:25.999031Z",
     "shell.execute_reply": "2025-03-21T01:55:25.998363Z",
     "shell.execute_reply.started": "2025-03-21T01:55:25.962041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset from the \"dataset\" folder\n",
    "tokenized_datasets = datasets.load_from_disk(\"dataset/tokenizers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T01:55:53.391100Z",
     "iopub.status.busy": "2025-03-21T01:55:53.390774Z",
     "iopub.status.idle": "2025-03-21T01:55:53.496338Z",
     "shell.execute_reply": "2025-03-21T01:55:53.495410Z",
     "shell.execute_reply.started": "2025-03-21T01:55:53.391072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = \"viet-news-sum-mt5-small-finetune\"\n",
    "# Define the training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    num_train_epochs=10, # 40 -> 50\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    predict_with_generate=True,\n",
    "    # fp16=True if torch.cuda.is_available() else False,\n",
    "    report_to=\"none\" # Disable wandb reporting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T01:55:59.499238Z",
     "iopub.status.busy": "2025-03-21T01:55:59.498932Z",
     "iopub.status.idle": "2025-03-21T10:29:01.934328Z",
     "shell.execute_reply": "2025-03-21T10:29:01.933645Z",
     "shell.execute_reply.started": "2025-03-21T01:55:59.499213Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-1efba77810de>:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55930' max='55930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55930/55930 8:33:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.027335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.023692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.019857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.060800</td>\n",
       "      <td>0.015207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.012891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.010073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.046400</td>\n",
       "      <td>0.009463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.046100</td>\n",
       "      <td>0.007866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.006860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.006372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=55930, training_loss=0.056790818105374, metrics={'train_runtime': 30781.5169, 'train_samples_per_second': 21.801, 'train_steps_per_second': 1.817, 'total_flos': 3.548280550588416e+17, 'train_loss': 0.056790818105374, 'epoch': 10.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['val'],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Finetune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T10:29:31.759313Z",
     "iopub.status.busy": "2025-03-21T10:29:31.759034Z",
     "iopub.status.idle": "2025-03-21T10:29:35.703152Z",
     "shell.execute_reply": "2025-03-21T10:29:35.702389Z",
     "shell.execute_reply.started": "2025-03-21T10:29:31.759293Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./viet-sum-mt5-small-finetune\n"
     ]
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model_path = \"viet-news-sum-mt5-small-finetune\"\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(model_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and tokenizer\n",
    "model_path = \"viet-news-sum-mt5-small-finetune\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "def preprocess_input(text):\n",
    "    inputs = tokenizer(text, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "# Define a function to generate the summary\n",
    "def generate_summary(text):\n",
    "    inputs = preprocess_input(text)\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=128, \n",
    "            early_stopping=True\n",
    "        )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-21T10:37:20.620767Z",
     "iopub.status.busy": "2025-03-21T10:37:20.620445Z",
     "iopub.status.idle": "2025-03-21T10:37:28.732981Z",
     "shell.execute_reply": "2025-03-21T10:37:28.732140Z",
     "shell.execute_reply.started": "2025-03-21T10:37:20.620746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Đến khoa gây mê hồi sức Bệnh viện Đức Giang , Hà Nội vừa tiếp nhận một nữ thai phụ mang thai 26 tuần bị viêm phổi , chấn thương sọ não nghiêm trọng với xuất huyết dưới nhện .\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "Vào ngày 8-1, khoa gây mê hồi sức Bệnh viện Đa khoa Đức Giang tiếp nhận bệnh nhân L.T.N.T. (23 tuổi, Chương Mỹ, Hà Nội) trong tình trạng hôn mê sau tai nạn giao thông.\n",
    "\n",
    "Thai phụ mang thai 26 tuần bị viêm phổi, chấn thương sọ não nghiêm trọng với xuất huyết dưới nhện và tụ máu dưới màng cứng trán phải.\n",
    "\n",
    "Theo bác sĩ Lê Nguyễn An - trưởng khoa gây mê hồi sức Bệnh viện Đa khoa Đức Giang, vấn đề thách thức trong quá trình điều trị với bệnh nhân này là việc cần phải đảm bảo sức khỏe cho cả mẹ và con là rất khó khăn.\n",
    "\n",
    "\"Các bác sĩ cố gắng duy trì tuổi thai ngoài 30 tuần để đảm bảo việc khi sinh ra trẻ có thể phát triển bình thường. Việc đảm bảo an toàn tính mạng cho mẹ cũng phải cân đối phù hợp, hạn chế tối thiểu việc ảnh hưởng tới thai nhi\", bác sĩ An nói.\n",
    "\n",
    "Trong suốt quá trình điều trị, các bác sĩ liên tục phối hợp với chuyên khoa sản và dinh dưỡng để đánh giá và điều chỉnh liên tục cho người bệnh để đảm bảo sự phát triển của em bé trong bụng mẹ.\n",
    "\n",
    "Đặc biệt việc chăm sóc người bệnh ở trạng thái hôn mê, thở qua mở khí quản rất khó khăn, nhiều nguy cơ rủi ro về tình trạng nhiễm khuẩn, thiếu hụt dinh dưỡng, loét trợt điểm tì đè, nguy cơ suy thai\".\n",
    "\n",
    "Sau 70 ngày điều trị, tình trạng của sản phụ dần ổn định. Các chỉ số sinh tồn cải thiện, bệnh nhân tự thở qua mở khí quản, thai phát triển bình thường.\n",
    "\n",
    "Tối 15-3, sản phụ có dấu hiệu chuyển dạ, thai 36 tuần (theo dự kiến sinh), ngôi ngược, ối vỡ sớm. Đội ngũ bác sĩ quyết định mổ lấy thai.\n",
    "\n",
    "Ca phẫu thuật thành công, một bé trai nặng 2kg chào đời khóc to, niêm mạc hồng hào trong niềm hạnh phúc vô bờ của đội ngũ y bác sĩ và gia đình.\n",
    "\n",
    "Ba ngày sau mổ, sản phụ tỉnh táo, tự ăn uống, được rút mở khí quản. Dự kiến cả mẹ và bé xuất viện trong ngày 21-3.\n",
    "\"\"\"\n",
    "\n",
    "input_text = input_text.replace(\"\\n\", \"\")\n",
    "\n",
    "summary = generate_summary(input_text)\n",
    "print(f\"Summary: {summary}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 228862993,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
